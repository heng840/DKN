import glob
import os
import json

from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM

def read_and_adapt_dataset(file_path):
    templama = {}
    with open(file_path, 'r') as f:
        for line in f:
            entry = json.loads(line)
            date_prefix = f"In {entry['date']},"
            formatted_query = date_prefix + " " + entry['query']
            # Modify the original 'query' to the formatted one with timestamp
            entry['query'] = formatted_query
            uuid = entry['id']
            templama[uuid] = entry
    return templama


def read_neuron_data(neurons_dir):
    neuron_data = {}
    result_files = glob.glob(os.path.join(neurons_dir, '*results*.json'))
    for file in result_files:
        with open(file, 'r') as f:
            data = json.load(f)
            neuron_data.update(data)
    return neuron_data


def calculate_accuracy_drop(initial_accuracy, new_accuracy):
    if initial_accuracy == 0 and new_accuracy == 0:
        return 0
    elif initial_accuracy == 0 and new_accuracy != 0:
        return float('inf')
    else:
        acc = (new_accuracy - initial_accuracy) / initial_accuracy
        return acc
def initiate_model_tokenizer(model_name, device='cuda'):
    if 'gpt2' in model_name:
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        model = GPT2LMHeadModel.from_pretrained(model_name)
        model = model.to(device)
    elif 'gpt-j' or 'llama' in model_name:
        model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
        # model = model.to(device)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer


def get_model_output(model, tokenizer, prompt, max_new_tokens=20, device='cuda'):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    prompt_length = inputs["input_ids"].shape[-1]

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=max_new_tokens
    )

    # Decode only the new tokens generated by the model
    response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)
    return response


def load_json_files_from_directory(dir_path, keyword=None):
    result = {}
    for filename in os.listdir(dir_path):
        if keyword is None:
            with open(os.path.join(dir_path, filename)) as file:
                data = json.load(file)
                result.update(data)
        else:
            if keyword in filename:
                with open(os.path.join(dir_path, filename)) as file:
                    data = json.load(file)
                    result.update(data)
    return result


def load_jsonl_files_from_directory(dir_path, keyword=None):
    result = {}
    for filename in os.listdir(dir_path):
        if keyword is None or keyword in filename:
            file_path = os.path.join(dir_path, filename)
            with open(file_path, 'r') as file:
                for line in file:
                    data = json.loads(line)
                    result.update(data)
    return result

